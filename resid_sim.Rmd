---
output: pdf_document
---

First off I generate the true values of 100 covariates. 
Although I use a random function to generate them, this is only for convenience. 
These could be generated through some kind of systematic procedure as long as the variable of interest (the first column) is unassociated with the others. In fact since I set a seed, that's exactly what I am actually doing.
To reiterate, what I am doing here is creating a population for the simulation. This should *not* be regarded as randomness being fundamentally involved in any way.

```{r, include = FALSE}

library(ggfortify)
library(patchwork)
library(tidyverse)
theme_set(theme_bw())

knitr::opts_chunk$set(warning=FALSE, message=FALSE)

```

```{r}
set.seed(100)
df <- as_tibble(matrix(runif(1000000), ncol = 100))
```



Now we want to extract the actual covariate of interest. This is the first column of the matrix. The other terms are all the terms that contribute to residual error. Attribute any kind of meaning to these things you want. What is important is that they will effect the resulting outcome variable in a deterministic way.
```{r}
x <- df %>% pull(1)
```


Now we create the outcome variable. This is assumed to be a perfectly deterministic linear function of all 100 covariates. The parameter for the covariate of interest is 6, and the other parameters are randomly generated. Again I want to reinforce I am generating them randomly for convenience, I could have enumerated the list of all 99 parameters systematically, but that is tedious. The whole model is still an entirely deterministic model. 


```{r}
noise_params <- runif(99, -0.3, 0.3)
y <- as.matrix(df) %*% c(6, noise_params)

```

The resulting plot looks like a picture perfect linear regression plot. 

```{r }
qplot(x,y)
```

We can go ahead and run a regression as well and see what happens. The estimated parameter value is approximately 6, and the residual plots all look perfect. 
```{r }
fit <- lm(y ~ x)
knitr::kable(broom::tidy(fit))
autoplot(fit)
```

So why does everything work out so nicely, and everything come out perfectly normal even though there is no randomness involved anywhere? To answer that question it's best to go back to very intro probability. Probability doesn't actually assume anything about randomness. Probability is actually about assigning self consistent measures to events in a sample space. When we run a linear regression, we condition on the covariate x, but we have actually marginalized over all the other 99 covariates involved. So for each point x we end up with a distribution of the relative frequencies of different y values. For each of the other covariates, call them $z_i$, there is an associated distribution of their contributions to y. Let's plot two of these distributions. 

```{r }
qplot(x = pull(df, 2) * noise_params[2], xlab = 'Noise variable 2', main = 'Histogram of the contribution to y from x_2')
qplot(x = pull(df, 3) * noise_params[3], xlab = 'Noise variable 3', main = 'Histogram of the contribution to y from x_3')
```

Both plots are roughly uniform distributions. Now when we add them all up together, a central limit theorem takes effect and the result is we get something very strongly resembling a normal distribution. We end up having an almost perfectly normal residual, and statistics works wonderfully despite everything actually being completely deterministic. 

```{r }
resid <- as.matrix(dplyr::select(df, 2:100)) %*% noise_params
qplot(resid, main = 'Residual histogram')
```


Above I did everything assuming everything is linear. Instead let's look at a case where the effect of our variable of interest is linear, but the residual is composed of a bunch of nonlinear sums. I generate random parameters and then represent the residual as \[\sum_{i=2}^{100}{f_i(\beta_i * x_i)}\] where the $f_i$ are non-linear functions. Each is one of sqrt(abs(x)), sin(x), tan(x), or log(abs(x)). I am going to add them all together to generate a residual term.


```{r}
y_base <- 6 * x
params <- runif(100, -0.03, 0.03)
functions <- c(function(.) sqrt(abs(.)), sin, tan, function(.) log(abs(.)))
functions <- rep(functions, times = 25)
```

First let's look at the first four components of the residual terms, since they're each generated by a different non-linearity. 

```{r}
wrap_elements((qplot(functions[[2]](params[2] * pull(df, 2)), xlab = '') + qplot(functions[[3]](params[3] * pull(df, 3)), xlab = '')) /
    (qplot(functions[[4]](params[4] * pull(df, 4)), xlab = '') + qplot(functions[[5]](params[5] * pull(df, 5)), xlab = ''))) + ggtitle('First four residual components')
```


Earlier the residual components, while not i.i.d, were just rescaled versions of the same distribution. The distributions are now clearly very different from each other. Let's look at the total residual.

```{r}
resid <- rep(0, length(y))
for (i in 2:100) {
  resid <- resid + functions[[i]](pull(df, i) * params[i])
}

qplot(resid, main = 'Histogram of Total Residual')
y <- y_base + resid
```

A central limit theorem still clearly applies, but the rate of convergence is a bit slower now. The resulting distribution is a bit skew-normal. This means our regression won't be perfect like before, but should still be pretty good and we can proceed and see how it looks.

```{r }
fit <- lm(y ~ x)
knitr::kable(broom::tidy(fit))
autoplot(fit)
```
